{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import email\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the enron email data set and we are trying to automatically\n",
    "# classify emails into sub-folders\n",
    "# we have the mid-size data set where each email has given its text,\n",
    "# subject, sender, CC, etc.\n",
    "# let's look at the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails = pd.read_csv('C:/Users/genaucka/Downloads/emails150MB/emails150MB.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  \\\n",
      "0  stokley-c/chris_stokley/projects/ees_/brenda_h...   \n",
      "1     stokley-c/chris_stokley/projects/ees_/ercot/1.   \n",
      "2    stokley-c/chris_stokley/projects/ees_/ercot/10.   \n",
      "3    stokley-c/chris_stokley/projects/ees_/ercot/11.   \n",
      "4     stokley-c/chris_stokley/projects/ees_/ercot/2.   \n",
      "5     stokley-c/chris_stokley/projects/ees_/ercot/3.   \n",
      "6     stokley-c/chris_stokley/projects/ees_/ercot/4.   \n",
      "7     stokley-c/chris_stokley/projects/ees_/ercot/5.   \n",
      "8     stokley-c/chris_stokley/projects/ees_/ercot/6.   \n",
      "9     stokley-c/chris_stokley/projects/ees_/ercot/7.   \n",
      "\n",
      "                                                   1  \n",
      "0  Message-ID: <32295689.1075858519201.JavaMail.e...  \n",
      "1  Message-ID: <11422706.1075858518802.JavaMail.e...  \n",
      "2  Message-ID: <24333819.1075858519016.JavaMail.e...  \n",
      "3  Message-ID: <17082846.1075858519041.JavaMail.e...  \n",
      "4  Message-ID: <16514884.1075858518826.JavaMail.e...  \n",
      "5  Message-ID: <3404255.1075858518849.JavaMail.ev...  \n",
      "6  Message-ID: <5007514.1075858518873.JavaMail.ev...  \n",
      "7  Message-ID: <6776582.1075858518897.JavaMail.ev...  \n",
      "8  Message-ID: <10287275.1075858518920.JavaMail.e...  \n",
      "9  Message-ID: <25807320.1075858518944.JavaMail.e...  \n",
      "                               0  \\\n",
      "59225  zufferli-j/sent_items/90.   \n",
      "59226  zufferli-j/sent_items/91.   \n",
      "59227  zufferli-j/sent_items/92.   \n",
      "59228  zufferli-j/sent_items/93.   \n",
      "59229  zufferli-j/sent_items/94.   \n",
      "59230  zufferli-j/sent_items/95.   \n",
      "59231  zufferli-j/sent_items/96.   \n",
      "59232  zufferli-j/sent_items/97.   \n",
      "59233  zufferli-j/sent_items/98.   \n",
      "59234  zufferli-j/sent_items/99.   \n",
      "\n",
      "                                                       1  \n",
      "59225  Message-ID: <25508136.1075842029796.JavaMail.e...  \n",
      "59226  Message-ID: <23829224.1075842029820.JavaMail.e...  \n",
      "59227  Message-ID: <25622034.1075842029843.JavaMail.e...  \n",
      "59228  Message-ID: <25507923.1075842029891.JavaMail.e...  \n",
      "59229  Message-ID: <20245656.1075842029914.JavaMail.e...  \n",
      "59230  Message-ID: <26807948.1075842029936.JavaMail.e...  \n",
      "59231  Message-ID: <25835861.1075842029959.JavaMail.e...  \n",
      "59232  Message-ID: <28979867.1075842029988.JavaMail.e...  \n",
      "59233  Message-ID: <22052556.1075842030013.JavaMail.e...  \n",
      "59234  Message-ID: <28618979.1075842030037.JavaMail.e...  \n",
      " \n",
      "size of the data set:\n",
      "(59235, 2)\n"
     ]
    }
   ],
   "source": [
    "print(mails.head(10))\n",
    "print(mails.tail(10))\n",
    "print(' ')\n",
    "print('size of the data set:')\n",
    "print(mails.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we have 59235 emails given\n",
    "# give some column names to work with\n",
    "mails.columns = ['file','message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this data set is not a ready-made data-frame\n",
    "# we will have to build it\n",
    "# specifically, we will need the label (i.e. the target, here the folders in which to classify)\n",
    "# and we will need predictors (text body, from )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10       stokley-c/chris_stokley/projects/ees_/ercot/8.\n",
       "150                   stokley-c/chris_stokley/sent/117.\n",
       "3000                                   sturm-f/sent/41.\n",
       "3500                           symes-k/_sent_mail/1070.\n",
       "40000                            weldon-c/sent_items/4.\n",
       "40500                      whalley-g/all_documents/287.\n",
       "Name: file, dtype: object"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's work at the target first: the classification into subfolders\n",
    "# the information is in the folder structure:\n",
    "mails.file.loc[[10,150,3000,3500,40000,40500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that quite some folders are of the \"sent\" type\n",
    "# these are not relevant for the task because we are to classify the incoming\n",
    "# emails\n",
    "mails = mails.loc[mails.file.str.find('sent') == -1]\n",
    "mails.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of only received emails:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46383"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have much less emails left\n",
    "print('Number of only received emails:')\n",
    "mails.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10       stokley-c/chris_stokley/projects/ees_/ercot/8.\n",
       "150           stokley-c/chris_stokley/volume_mang_/116.\n",
       "2000                              swerzbin-m/inbox/100.\n",
       "2400                        symes-k/all_documents/1139.\n",
       "3000                        symes-k/all_documents/1785.\n",
       "3500                        symes-k/all_documents/2304.\n",
       "4000                        symes-k/all_documents/2840.\n",
       "4500                        symes-k/all_documents/3387.\n",
       "5000                         symes-k/all_documents/477.\n",
       "8000                   symes-k/discussion_threads/2184.\n",
       "10000                                   symes-k/it/105.\n",
       "12000                      taylor-m/all_documents/2403.\n",
       "33000                       whalley-l/all_documents/81.\n",
       "36000                        white-s/deleted_items/692.\n",
       "40000                williams-w3/bill_williams_iii/636.\n",
       "Name: file, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the folder structures again\n",
    "mails.file.loc[[10,150,2000,2400,3000,3500,4000,4500,5000,8000,10000,12000,33000,36000,40000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10442       taylor-m/all_documents/1.\n",
      "10443      taylor-m/all_documents/10.\n",
      "10444     taylor-m/all_documents/100.\n",
      "10445    taylor-m/all_documents/1000.\n",
      "10446    taylor-m/all_documents/1001.\n",
      "10447    taylor-m/all_documents/1002.\n",
      "10448    taylor-m/all_documents/1003.\n",
      "10449    taylor-m/all_documents/1004.\n",
      "10450    taylor-m/all_documents/1005.\n",
      "10451    taylor-m/all_documents/1006.\n",
      "10452    taylor-m/all_documents/1007.\n",
      "10453    taylor-m/all_documents/1008.\n",
      "10454    taylor-m/all_documents/1009.\n",
      "10455     taylor-m/all_documents/101.\n",
      "10456    taylor-m/all_documents/1010.\n",
      "10457    taylor-m/all_documents/1011.\n",
      "10458    taylor-m/all_documents/1012.\n",
      "10459    taylor-m/all_documents/1013.\n",
      "10460    taylor-m/all_documents/1014.\n",
      "10461    taylor-m/all_documents/1015.\n",
      "10462    taylor-m/all_documents/1016.\n",
      "10463    taylor-m/all_documents/1017.\n",
      "10464    taylor-m/all_documents/1018.\n",
      "10465    taylor-m/all_documents/1019.\n",
      "10466     taylor-m/all_documents/102.\n",
      "10467    taylor-m/all_documents/1020.\n",
      "10468    taylor-m/all_documents/1021.\n",
      "10469    taylor-m/all_documents/1022.\n",
      "10470    taylor-m/all_documents/1023.\n",
      "10471    taylor-m/all_documents/1024.\n",
      "Name: file, dtype: object\n",
      "21878        taylor-m/travel/91.\n",
      "21879     taylor-m/uk_trading/1.\n",
      "21880    taylor-m/uk_trading/10.\n",
      "21881    taylor-m/uk_trading/11.\n",
      "21882    taylor-m/uk_trading/12.\n",
      "21883    taylor-m/uk_trading/13.\n",
      "21884    taylor-m/uk_trading/14.\n",
      "21885    taylor-m/uk_trading/15.\n",
      "21886    taylor-m/uk_trading/16.\n",
      "21887    taylor-m/uk_trading/17.\n",
      "21888    taylor-m/uk_trading/18.\n",
      "21889    taylor-m/uk_trading/19.\n",
      "21890     taylor-m/uk_trading/2.\n",
      "21891    taylor-m/uk_trading/20.\n",
      "21892     taylor-m/uk_trading/3.\n",
      "21893     taylor-m/uk_trading/4.\n",
      "21894     taylor-m/uk_trading/5.\n",
      "21895     taylor-m/uk_trading/6.\n",
      "21896     taylor-m/uk_trading/7.\n",
      "21897     taylor-m/uk_trading/8.\n",
      "21898     taylor-m/uk_trading/9.\n",
      "21899          taylor-m/yoder/1.\n",
      "21900         taylor-m/yoder/13.\n",
      "21901         taylor-m/yoder/14.\n",
      "21902         taylor-m/yoder/15.\n",
      "21903          taylor-m/yoder/2.\n",
      "21904          taylor-m/yoder/3.\n",
      "21905          taylor-m/yoder/4.\n",
      "21906          taylor-m/yoder/5.\n",
      "21907          taylor-m/yoder/6.\n",
      "Name: file, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# we can see here that users (always the first string before first '/')\n",
    "# have all their personal way of sorting emails into their personal subfolders\n",
    "# there are two ways now to proceed:\n",
    "\n",
    "# find a GENERAL rule for classifiying emails that fits all employees:\n",
    "# for this we'd need to find the GENERAL subfolders (e.g. 'finance', 'projects', 'personal')\n",
    "# for this we'd need some unsupervised learning to find clusters of emails\n",
    "# name those clusters; use them as labels and from this build a classfier...\n",
    "# I do not think this is a good idea, because it means \"one-size-fits-all\"\n",
    "# the categories will be blurry and won't really help the single employee\n",
    "\n",
    "# the second way is to find PERSONAL rules for classifying the emails\n",
    "# for this we'd assume that every person has their own way of sorting emails\n",
    "# the email client (me now) will automatically learn how they classify\n",
    "# the labels are then given by their wubfolder structure\n",
    "# I think this is more helpful for every single person\n",
    "# but it only works if we have sufficient emails per subject\n",
    "# also the suggestion to automatically classify in a subfolder should only\n",
    "# be made if a certain threshold performance is reached\n",
    "# let's check the emails of a single person\n",
    "print(mails.file.loc[mails.file.str.find('taylor-m') != -1].head(30))\n",
    "print(mails.file.loc[mails.file.str.find('taylor-m') != -1].tail(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, so to keep it simple we will assume that the label\n",
    "# is the folder path after the user name and before the number\n",
    "# I started to implement the parsing myself but given the time constraint\n",
    "# and given that the ENRON email data set is very well known and widely used\n",
    "# on KAGGLE I turned to a ready made parsing solution using the python email package:\n",
    "# https://www.kaggle.com/jaykrishna/topic-modeling-enron-email-dataset\n",
    "# it helps especially to get the message ID and the content separated\n",
    "\n",
    "# we can see above already, that there are certain folders that are very repetitive and uninformative:\n",
    "# \"all_documents\"; similarly some users have \"inbox\"\n",
    "# this will probably lead to problems down the road; but we first try out a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is from: https://www.kaggle.com/jaykrishna/topic-modeling-enron-email-dataset\n",
    "## Helper functions\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>file</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Cc</th>\n",
       "      <th>Mime-Version</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Bcc</th>\n",
       "      <th>X-From</th>\n",
       "      <th>X-To</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>content</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stokley-c/chris_stokley/projects/ees_/brenda_h...</td>\n",
       "      <td>&lt;32295689.1075858519201.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Sun, 22 Jul 2001 16:29:07 -0700 (PDT)</td>\n",
       "      <td>(f..herod@enron.com)</td>\n",
       "      <td>(chris.stokley@enron.com, kenny.ha@enron.com)</td>\n",
       "      <td>FW: Metered Usage</td>\n",
       "      <td>vivian.hart@enron.com, patti.thompson@enron.com</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>vivian.hart@enron.com, patti.thompson@enron.com</td>\n",
       "      <td>Herod, Brenda F. &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>Ha, Kenny &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=Not...</td>\n",
       "      <td>Hart, Vivian &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=...</td>\n",
       "      <td></td>\n",
       "      <td>\\Stokley, Chris (Non-Privileged)\\Chris Stokley...</td>\n",
       "      <td>Stokley-C</td>\n",
       "      <td>Stokley, Chris (Non-Privileged).pst</td>\n",
       "      <td>As part of Project Ranger, it is my goal to ha...</td>\n",
       "      <td>stokley-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>stokley-c/chris_stokley/projects/ees_/ercot/1.</td>\n",
       "      <td>&lt;11422706.1075858518802.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Tue, 24 Jul 2001 08:19:57 -0700 (PDT)</td>\n",
       "      <td>(preston.ochsner@enron.com)</td>\n",
       "      <td>(george.phillips@enron.com, chris.stokley@enro...</td>\n",
       "      <td>Settlements - What we've done in CA and what w...</td>\n",
       "      <td>gary.nelson@enron.com, jeffrey.miller@enron.co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>gary.nelson@enron.com, jeffrey.miller@enron.co...</td>\n",
       "      <td>Ochsner, Preston &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>Stokley, Chris &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/C...</td>\n",
       "      <td>Nelson, Gary &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=...</td>\n",
       "      <td></td>\n",
       "      <td>\\Stokley, Chris (Non-Privileged)\\Chris Stokley...</td>\n",
       "      <td>Stokley-C</td>\n",
       "      <td>Stokley, Chris (Non-Privileged).pst</td>\n",
       "      <td>EB640\\t</td>\n",
       "      <td>stokley-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>stokley-c/chris_stokley/projects/ees_/ercot/10.</td>\n",
       "      <td>&lt;24333819.1075858519016.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Wed, 1 Aug 2001 06:07:35 -0700 (PDT)</td>\n",
       "      <td>(preston.ochsner@enron.com)</td>\n",
       "      <td>(p..o'neil@enron.com, george.phillips@enron.co...</td>\n",
       "      <td>FW: Meeting Thurs. 8/2 on Texas retail issues</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Ochsner, Preston &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>Phillips, George &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\Stokley, Chris (Non-Privileged)\\Chris Stokley...</td>\n",
       "      <td>Stokley-C</td>\n",
       "      <td>Stokley, Chris (Non-Privileged).pst</td>\n",
       "      <td>would like to have you there\\n\\n--------------...</td>\n",
       "      <td>stokley-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stokley-c/chris_stokley/projects/ees_/ercot/11.</td>\n",
       "      <td>&lt;17082846.1075858519041.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 30 Jul 2001 18:04:46 -0700 (PDT)</td>\n",
       "      <td>(preston.ochsner@enron.com)</td>\n",
       "      <td>(george.phillips@enron.com, michele.raque@enro...</td>\n",
       "      <td>ERCOT Physical Delivery Progress Report</td>\n",
       "      <td>joseph.wagner@enron.com, gary.nelson@enron.com</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>joseph.wagner@enron.com, gary.nelson@enron.com</td>\n",
       "      <td>Ochsner, Preston &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>O'Neil, Murray P. &lt;/O=ENRON/OU=NA/CN=RECIPIENT...</td>\n",
       "      <td>Wagner, Joseph &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/C...</td>\n",
       "      <td></td>\n",
       "      <td>\\Stokley, Chris (Non-Privileged)\\Chris Stokley...</td>\n",
       "      <td>Stokley-C</td>\n",
       "      <td>Stokley, Chris (Non-Privileged).pst</td>\n",
       "      <td>\\nI need your help to make September 1 physica...</td>\n",
       "      <td>stokley-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>stokley-c/chris_stokley/projects/ees_/ercot/2.</td>\n",
       "      <td>&lt;16514884.1075858518826.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Tue, 24 Jul 2001 18:01:37 -0700 (PDT)</td>\n",
       "      <td>(preston.ochsner@enron.com)</td>\n",
       "      <td>(george.phillips@enron.com, jeff.merola@enron....</td>\n",
       "      <td>ERCOT Physical Delivery Progress Report</td>\n",
       "      <td>jeffrey.miller@enron.com, joseph.wagner@enron....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>jeffrey.miller@enron.com, joseph.wagner@enron....</td>\n",
       "      <td>Ochsner, Preston &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>O'Neil, Murray P. &lt;/O=ENRON/OU=NA/CN=RECIPIENT...</td>\n",
       "      <td>Miller, Jeffrey &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/...</td>\n",
       "      <td></td>\n",
       "      <td>\\Stokley, Chris (Non-Privileged)\\Chris Stokley...</td>\n",
       "      <td>Stokley-C</td>\n",
       "      <td>Stokley, Chris (Non-Privileged).pst</td>\n",
       "      <td>\\nWhile cautious of the issues we face, I'm ve...</td>\n",
       "      <td>stokley-c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               file  \\\n",
       "0      0  stokley-c/chris_stokley/projects/ees_/brenda_h...   \n",
       "1      1     stokley-c/chris_stokley/projects/ees_/ercot/1.   \n",
       "2      2    stokley-c/chris_stokley/projects/ees_/ercot/10.   \n",
       "3      3    stokley-c/chris_stokley/projects/ees_/ercot/11.   \n",
       "4      4     stokley-c/chris_stokley/projects/ees_/ercot/2.   \n",
       "\n",
       "                                      Message-ID  \\\n",
       "0  <32295689.1075858519201.JavaMail.evans@thyme>   \n",
       "1  <11422706.1075858518802.JavaMail.evans@thyme>   \n",
       "2  <24333819.1075858519016.JavaMail.evans@thyme>   \n",
       "3  <17082846.1075858519041.JavaMail.evans@thyme>   \n",
       "4  <16514884.1075858518826.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    Date                         From  \\\n",
       "0  Sun, 22 Jul 2001 16:29:07 -0700 (PDT)         (f..herod@enron.com)   \n",
       "1  Tue, 24 Jul 2001 08:19:57 -0700 (PDT)  (preston.ochsner@enron.com)   \n",
       "2   Wed, 1 Aug 2001 06:07:35 -0700 (PDT)  (preston.ochsner@enron.com)   \n",
       "3  Mon, 30 Jul 2001 18:04:46 -0700 (PDT)  (preston.ochsner@enron.com)   \n",
       "4  Tue, 24 Jul 2001 18:01:37 -0700 (PDT)  (preston.ochsner@enron.com)   \n",
       "\n",
       "                                                  To  \\\n",
       "0      (chris.stokley@enron.com, kenny.ha@enron.com)   \n",
       "1  (george.phillips@enron.com, chris.stokley@enro...   \n",
       "2  (p..o'neil@enron.com, george.phillips@enron.co...   \n",
       "3  (george.phillips@enron.com, michele.raque@enro...   \n",
       "4  (george.phillips@enron.com, jeff.merola@enron....   \n",
       "\n",
       "                                             Subject  \\\n",
       "0                                  FW: Metered Usage   \n",
       "1  Settlements - What we've done in CA and what w...   \n",
       "2      FW: Meeting Thurs. 8/2 on Texas retail issues   \n",
       "3            ERCOT Physical Delivery Progress Report   \n",
       "4            ERCOT Physical Delivery Progress Report   \n",
       "\n",
       "                                                  Cc Mime-Version  \\\n",
       "0    vivian.hart@enron.com, patti.thompson@enron.com          1.0   \n",
       "1  gary.nelson@enron.com, jeffrey.miller@enron.co...          1.0   \n",
       "2                                               None          1.0   \n",
       "3     joseph.wagner@enron.com, gary.nelson@enron.com          1.0   \n",
       "4  jeffrey.miller@enron.com, joseph.wagner@enron....          1.0   \n",
       "\n",
       "                   Content-Type  ...  \\\n",
       "0  text/plain; charset=us-ascii  ...   \n",
       "1  text/plain; charset=us-ascii  ...   \n",
       "2  text/plain; charset=us-ascii  ...   \n",
       "3  text/plain; charset=us-ascii  ...   \n",
       "4  text/plain; charset=us-ascii  ...   \n",
       "\n",
       "                                                 Bcc  \\\n",
       "0    vivian.hart@enron.com, patti.thompson@enron.com   \n",
       "1  gary.nelson@enron.com, jeffrey.miller@enron.co...   \n",
       "2                                               None   \n",
       "3     joseph.wagner@enron.com, gary.nelson@enron.com   \n",
       "4  jeffrey.miller@enron.com, joseph.wagner@enron....   \n",
       "\n",
       "                                              X-From  \\\n",
       "0  Herod, Brenda F. </O=ENRON/OU=NA/CN=RECIPIENTS...   \n",
       "1  Ochsner, Preston </O=ENRON/OU=NA/CN=RECIPIENTS...   \n",
       "2  Ochsner, Preston </O=ENRON/OU=NA/CN=RECIPIENTS...   \n",
       "3  Ochsner, Preston </O=ENRON/OU=NA/CN=RECIPIENTS...   \n",
       "4  Ochsner, Preston </O=ENRON/OU=NA/CN=RECIPIENTS...   \n",
       "\n",
       "                                                X-To  \\\n",
       "0  Ha, Kenny </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Not...   \n",
       "1  Stokley, Chris </O=ENRON/OU=NA/CN=RECIPIENTS/C...   \n",
       "2  Phillips, George </O=ENRON/OU=NA/CN=RECIPIENTS...   \n",
       "3  O'Neil, Murray P. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
       "4  O'Neil, Murray P. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
       "\n",
       "                                                X-cc X-bcc  \\\n",
       "0  Hart, Vivian </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...         \n",
       "1  Nelson, Gary </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...         \n",
       "2                                                            \n",
       "3  Wagner, Joseph </O=ENRON/OU=NA/CN=RECIPIENTS/C...         \n",
       "4  Miller, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/...         \n",
       "\n",
       "                                            X-Folder   X-Origin  \\\n",
       "0  \\Stokley, Chris (Non-Privileged)\\Chris Stokley...  Stokley-C   \n",
       "1  \\Stokley, Chris (Non-Privileged)\\Chris Stokley...  Stokley-C   \n",
       "2  \\Stokley, Chris (Non-Privileged)\\Chris Stokley...  Stokley-C   \n",
       "3  \\Stokley, Chris (Non-Privileged)\\Chris Stokley...  Stokley-C   \n",
       "4  \\Stokley, Chris (Non-Privileged)\\Chris Stokley...  Stokley-C   \n",
       "\n",
       "                            X-FileName  \\\n",
       "0  Stokley, Chris (Non-Privileged).pst   \n",
       "1  Stokley, Chris (Non-Privileged).pst   \n",
       "2  Stokley, Chris (Non-Privileged).pst   \n",
       "3  Stokley, Chris (Non-Privileged).pst   \n",
       "4  Stokley, Chris (Non-Privileged).pst   \n",
       "\n",
       "                                             content       user  \n",
       "0  As part of Project Ranger, it is my goal to ha...  stokley-c  \n",
       "1                                            EB640\\t  stokley-c  \n",
       "2  would like to have you there\\n\\n--------------...  stokley-c  \n",
       "3  \\nI need your help to make September 1 physica...  stokley-c  \n",
       "4  \\nWhile cautious of the issues we face, I'm ve...  stokley-c  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is from: https://www.kaggle.com/jaykrishna/topic-modeling-enron-email-dataset\n",
    "\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, mails['message']))\n",
    "mails.drop('message', axis=1, inplace=True)\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    mails[key] = [doc[key] for doc in messages]\n",
    "# Parse content from emails\n",
    "mails['content'] = list(map(get_text_from_email, messages))\n",
    "# Split multiple email addresses\n",
    "mails['From'] = mails['From'].map(split_email_addresses)\n",
    "mails['To'] = mails['To'].map(split_email_addresses)\n",
    "\n",
    "# Extract the root of 'file' as 'user'\n",
    "mails['user'] = mails['file'].map(lambda x:x.split('/')[0])\n",
    "del messages\n",
    "\n",
    "mails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the user variable, but we need the subfolder variable as well\n",
    "def get_subfolder(cur_str):\n",
    "    parts = cur_str.split('/')\n",
    "    return('/'.join(parts[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails['subfolder'] = mails.file.apply(get_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    chris_stokley/projects/ees_/brenda_herod\n",
       "1           chris_stokley/projects/ees_/ercot\n",
       "2           chris_stokley/projects/ees_/ercot\n",
       "3           chris_stokley/projects/ees_/ercot\n",
       "4           chris_stokley/projects/ees_/ercot\n",
       "5           chris_stokley/projects/ees_/ercot\n",
       "6           chris_stokley/projects/ees_/ercot\n",
       "7           chris_stokley/projects/ees_/ercot\n",
       "8           chris_stokley/projects/ees_/ercot\n",
       "9           chris_stokley/projects/ees_/ercot\n",
       "Name: subfolder, dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails.subfolder[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 23.000000 users.\n",
      " \n",
      "We have per user that much data:\n",
      "taylor-m       11466\n",
      "symes-k         8178\n",
      "williams-w3     2921\n",
      "whalley-l       2883\n",
      "white-s         2815\n",
      "watson-k        1979\n",
      "whalley-g       1748\n",
      "ward-k          1712\n",
      "wolfe-j         1494\n",
      "zipper-a        1213\n",
      "tholt-j         1201\n",
      "ybarbo-p        1161\n",
      "thomas-p        1130\n",
      "weldon-c        1110\n",
      "williams-j      1084\n",
      "storey-g         876\n",
      "sturm-f          752\n",
      "tycholiz-b       737\n",
      "townsend-j       564\n",
      "whitt-m          504\n",
      "stokley-c        354\n",
      "swerzbin-m       282\n",
      "zufferli-j       219\n",
      "Name: user, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# so let's check how many users we have got and how many emails per user\n",
    "all_users = set(mails.user)\n",
    "print('We have {:f} users.'.format(len(all_users)))\n",
    "print(' ')\n",
    "print('We have per user that much data:')\n",
    "print(mails.user.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in some users it can be tricky (less than 500 messages), but we will see\n",
    "# this will further lead to very few messages in deep subfolders which makes it\n",
    "# hard for a classifier (too few examples)\n",
    "\n",
    "# let's turn to the message\n",
    "# we are given a message ID, most importantly per employee,\n",
    "# we need to know whether any message has been placed into two different folders\n",
    "# i.e. duplicates check\n",
    "def no_duplicates(cur_df):\n",
    "    duplicateRowsDF = cur_df[cur_df.duplicated(['Message-ID'])]\n",
    "    if (duplicateRowsDF.shape[0]>0):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are duplicate problems!\n"
     ]
    }
   ],
   "source": [
    "# run for all users\n",
    "dup = []\n",
    "for u in all_users:\n",
    "    cur_df = mails.loc[mails.user == u]\n",
    "    dup.append(discard_duplicates(cur_df))\n",
    "    \n",
    "if all(dup):\n",
    "    print('There are no duplicate problems.')\n",
    "else:\n",
    "    print('There are duplicate problems!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification into folders two variables seem most important:\n",
    "# From whom is the email?\n",
    "# What is the content??\n",
    "# we merge this into one information \"soup\"\n",
    "def create_soup(x):\n",
    "    return ' '.join(list(x['From'])) + ' ' + x['content']\n",
    "\n",
    "mails['soup'] = mails.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     f..herod@enron.com As part of Project Ranger, ...\n",
       "1                     preston.ochsner@enron.com EB640\\t\n",
       "2     preston.ochsner@enron.com would like to have y...\n",
       "3     preston.ochsner@enron.com \\nI need your help t...\n",
       "4     preston.ochsner@enron.com \\nWhile cautious of ...\n",
       "5     preston.ochsner@enron.com as promised. \\n\\nonc...\n",
       "6     preston.ochsner@enron.com as promised, below a...\n",
       "7     preston.ochsner@enron.com see you there\\n-----...\n",
       "8     preston.ochsner@enron.com fyi\\n---------------...\n",
       "9     preston.ochsner@enron.com can we meet Wednesda...\n",
       "10    preston.ochsner@enron.com did you guys ever fi...\n",
       "11    preston.ochsner@enron.com have you had a chanc...\n",
       "12    george.phillips@enron.com Per our discussion l...\n",
       "13    neil.bresnan@enron.com \\n---------------------...\n",
       "14    neil.bresnan@enron.com Is EPMI involved with t...\n",
       "15    edith.cross@enron.com We also need to add a Ph...\n",
       "16    edith.cross@enron.com Here is a tentative list...\n",
       "17    edith.cross@enron.com Please plan to attend a ...\n",
       "18    jeffrey.jackson@enron.com Here are a few of th...\n",
       "19    beth.apollo@enron.com \\n\\nDear all,\\n\\n\\nJust ...\n",
       "Name: soup, dtype: object"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails.soup[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function I have adapted from a email spam classification\n",
    "# I could use it for stemming (cause it is not in sklearn by default)\n",
    "# but first try without\n",
    "def process_message(message, lower_case = True, stem = True, stop_words = True, gram = 1):\n",
    "    '''function to pre-process a message'''\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "            return w # early return\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the classification for a subject\n",
    "cur_df = mails.loc[mails.user == 'taylor-m']\n",
    "\n",
    "# predictors X and target y\n",
    "X = cur_df.soup\n",
    "y = cur_df.subfolder\n",
    "\n",
    "\n",
    "# split in train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2867, 4000)\n"
     ]
    }
   ],
   "source": [
    "# from here on we will use the sklearn\n",
    "# which does not seem to have stemming, but for now we won't use it\n",
    "# form my experience stemming does not always increase performance\n",
    "\n",
    "# vectorizing\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=4000,strip_accents='unicode',lowercase=True)\n",
    "\n",
    "# Construct the required TF-IDF matrix by fitting the transformation model\n",
    "# and then applying the transformation\n",
    "vectorizer.fit(X_train)\n",
    "vect_matrix_train = vectorizer.transform(X_train)\n",
    "vect_matrix_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Output the shape of tfidf_matrix of test data set\n",
    "print(vect_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier and predict the training and test data\n",
    "# maybe using complement NB to deal with unbalancedness\n",
    "#clf = MultinomialNB()\n",
    "clf = ComplementNB()\n",
    "#clf = GaussianNB()\n",
    "clf.fit(vect_matrix_train, y_train)\n",
    "preds_ts = clf.predict(vect_matrix_test)\n",
    "preds_tr = clf.predict(vect_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_documents',\n",
       " 'archive',\n",
       " 'archive/11_99',\n",
       " 'archive/1_00',\n",
       " 'archive/2001_07',\n",
       " 'archive/2001_08',\n",
       " 'archive/2001_10',\n",
       " 'archive/2001_11',\n",
       " 'archive/5_00',\n",
       " 'archive/6_00',\n",
       " 'archive/7_00',\n",
       " 'archive/8_00',\n",
       " 'archive/9_00',\n",
       " 'archive/november1999',\n",
       " 'brazil_trading',\n",
       " 'credit_watch_list',\n",
       " 'deleted_items',\n",
       " 'inbox',\n",
       " 'inbox/compression',\n",
       " 'inbox/entouch',\n",
       " 'inbox/esource',\n",
       " 'inbox/ev',\n",
       " 'inbox/john_mas',\n",
       " 'inbox/lisa_j',\n",
       " 'inbox/marketing',\n",
       " 'inbox/mary_schoen',\n",
       " 'inbox/methanol_plant',\n",
       " 'inbox/nox_model',\n",
       " 'inbox/parking',\n",
       " 'inbox/passwords',\n",
       " 'inbox/recruiting',\n",
       " 'inbox/siegal',\n",
       " 'inbox/so2',\n",
       " 'inbox/social',\n",
       " 'inbox/tnrcc',\n",
       " 'inbox/training',\n",
       " 'inbox/trevor',\n",
       " 'kelly',\n",
       " 'kiodex',\n",
       " 'monmouth',\n",
       " 'notes_inbox',\n",
       " 'online_trading',\n",
       " 'online_trading/content',\n",
       " 'online_trading/credit_derivatives',\n",
       " 'online_trading/no_more_confirms',\n",
       " 'online_trading/product_descriptions',\n",
       " 'restricted_list',\n",
       " 'stored_messages',\n",
       " 'swap_group_project_lists',\n",
       " 'time_off',\n",
       " 'travel',\n",
       " 'uk_trading',\n",
       " 'yoder'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if classifier makes varied predictions\n",
    "set(preds_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now only use accuracy to ball-park\n",
    "# since it is a multinomial problem accuracy is a good first-look measure \n",
    "def report(y,y_hat):\n",
    "    '''function to print some metrics'''\n",
    "    acc = metrics.accuracy_score(y,y_hat)\n",
    "    print('Accuracy: {:f}'.format(acc))\n",
    "    \n",
    "    # also give the accuracy under 0-hypothesis\n",
    "    y_s = pd.Series(y_s)\n",
    "    acc_0 = 0\n",
    "    for i in range(100):\n",
    "        acc_0 += metrics.accuracy_score(y_hat_s.sample(frac=1),y_hat_s)\n",
    "    \n",
    "    print('Accuracy under assumption of null-hypothesis: {:f}'.format(acc_0/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Train:\n",
      "Accuracy: 0.294569\n",
      "Accuracy under assumption of null-hypothesis: 0.337583\n",
      " \n",
      "Performance Test:\n",
      "Accuracy: 0.307290\n",
      "Accuracy under assumption of null-hypothesis: 0.391751\n"
     ]
    }
   ],
   "source": [
    "# evaluation train and test\n",
    "print('Performance Train:')\n",
    "report(y_train, preds_tr)\n",
    "print(' ')\n",
    "print('Performance Test:')\n",
    "report(y_test, preds_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier performes not at all\n",
    "# probelmatic is that the labels are heavily multinnomial and hiearchical\n",
    "# i.e. an email that is in 'inbox/compression' cannot be in 'archive/2001_11'\n",
    "# perhaps a decision tree is more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# first try a different one\n",
    "clf = LogisticRegression(multi_class = 'multinomial', solver='saga')\n",
    "clf.fit(vect_matrix_train, y_train)\n",
    "preds_ts = clf.predict(vect_matrix_test)\n",
    "preds_tr = clf.predict(vect_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Train:\n",
      "Accuracy: 0.565415\n",
      " \n",
      "Performance Test:\n",
      "Accuracy: 0.396931\n"
     ]
    }
   ],
   "source": [
    "# evaluation train and test\n",
    "print('Performance Train:')\n",
    "report(y_train, preds_tr)\n",
    "print(' ')\n",
    "print('Performance Test:')\n",
    "report(y_test, preds_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also very badly\n",
    "# let's try the random forest\n",
    "clf = RandomForestClassifier(n_estimators = 100,max_features = 0.1)\n",
    "clf.fit(vect_matrix_train, y_train)\n",
    "preds_ts = clf.predict(vect_matrix_test)\n",
    "preds_tr = clf.predict(vect_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Train:\n",
      "Accuracy: 0.697756\n",
      "Accuracy under assumption of null-hypothesis: 0.319016\n",
      " \n",
      "Performance Test:\n",
      "Accuracy: 0.307290\n",
      "Accuracy under assumption of null-hypothesis: 0.374437\n"
     ]
    }
   ],
   "source": [
    "# evaluation train and test\n",
    "print('Performance Train:')\n",
    "report(y_train, preds_tr)\n",
    "print(' ')\n",
    "print('Performance Test:')\n",
    "report(y_test, preds_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the random forest at least fits the training data, but overfits\n",
    "# it does not generalize well to new data; it even over-learns;\n",
    "# i.e. it learns things in the training data that are completely\n",
    "# different in the test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I am turning online to find out more about this problem\n",
    "# there is a paper\n",
    "# let's see how these colleagues dealt with some of the issues:\n",
    "# https://www.semanticscholar.org/paper/...\n",
    "# ...Automatic-Categorization-of-Email-into-Folders%3A-on-Bekkerman/b52e782f894e9d0223618db6c01aa381627ff61b\n",
    "\n",
    "# okay in this paper the people also complain about the deep, small folders and the repetitive, uninformative folders\n",
    "# that were not created by the employees\n",
    "# they disregard those; how exactly to do this is unclear to me: disregard completely those messages; or just\n",
    "# remove the folder from path? \"All_Documents/project_nox\" --> here All_documents is uninformative but project nox is\n",
    "\n",
    "# I would create a new subfolder variable: i.e. messages in very deep but poorly populated folders will be assgined\n",
    "# the folder one level up: this way messages do not get lost\n",
    "\n",
    "# further they argue that the train and test split should be time-dependent: i.e. the classifier should only\n",
    "# learn form past emails and be only assessed on future emails\n",
    "\n",
    "# They use the bag-of-word method like me (i.e. creating a word soup) and turning the\n",
    "# each soup in to a vector that counts word occurences\n",
    "# they prune some words\n",
    "\n",
    "# They use maximum entropy with quasi newton optimization,\n",
    "# Naive Bayes (as suggested by me already); support vector machine (I often end up\n",
    "# not using because too slow in training) and Wide-Margin Winnow, an online \n",
    "# learning algo that can deal with high dimensionality data; do not know it yet but sounds interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's implement the subfolder cleaning\n",
    "# I keep the original one because the mapping back to original labels\n",
    "# can help later on\n",
    "mails['subf_clean'] = mails.subfolder\n",
    "mails.subf_clean = mails.subf_clean.str.lower()\n",
    "mails.subf_clean = mails.subf_clean.str.replace('all_documents/','')\n",
    "mails.subf_clean = mails.subf_clean.str.replace('inbox/','')\n",
    "mails.subf_clean = mails.subf_clean.str.replace('discussion_threads/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now discard emails that are in folders with just 3 or less emails\n",
    "def create_full_folder(x):\n",
    "    return x['user']+'/' +x['subf_clean']\n",
    "\n",
    "mails['full_folder'] = mails.apply(create_full_folder, axis=1)\n",
    "freqs = mails['full_folder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here I would continue the cleaning of the labels and then retry naive bayes and random forest; and then also the other\n",
    "# ones suggested in the paper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue after 2pm submission:\n",
    "frqs = pd.DataFrame(mails.subf_clean.value_counts().reset_index())\n",
    "frqs.columns = ['subf_clean', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails = pd.merge(mails, frqs, on='subf_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails = mails.loc[mails['count'] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the classification for a subject\n",
    "cur_df = mails.loc[mails.user == 'taylor-m']\n",
    "\n",
    "# predictors X and target y\n",
    "X = cur_df.soup\n",
    "y = cur_df.subf_clean\n",
    "\n",
    "# split in train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_documents',\n",
       " 'analyst_prog',\n",
       " 'archive',\n",
       " 'archive/11_99',\n",
       " 'archive/1_00',\n",
       " 'archive/2001_05',\n",
       " 'archive/2001_06',\n",
       " 'archive/2001_07',\n",
       " 'archive/2001_08',\n",
       " 'archive/2001_09',\n",
       " 'archive/2001_10',\n",
       " 'archive/2001_11',\n",
       " 'archive/2_00',\n",
       " 'archive/5_00',\n",
       " 'archive/6_00',\n",
       " 'archive/7_00',\n",
       " 'archive/8_00',\n",
       " 'archive/9_00',\n",
       " 'archive/november1999',\n",
       " 'australia_trading',\n",
       " 'contacts',\n",
       " 'credit_watch_list',\n",
       " 'deleted_items',\n",
       " 'entouch',\n",
       " 'esource',\n",
       " 'ev',\n",
       " 'inbox',\n",
       " 'isda',\n",
       " 'kelly',\n",
       " 'kiodex',\n",
       " 'lisa_j',\n",
       " 'marketing',\n",
       " 'mary_schoen',\n",
       " 'methanol_plant',\n",
       " 'monmouth',\n",
       " 'notes_inbox',\n",
       " 'nox_model',\n",
       " 'nox_model/websites',\n",
       " 'online_trading',\n",
       " 'online_trading/content',\n",
       " 'online_trading/credit_derivatives',\n",
       " 'online_trading/eta_amendments',\n",
       " 'online_trading/no_more_confirms',\n",
       " 'online_trading/product_descriptions',\n",
       " 'oslo',\n",
       " 'passwords',\n",
       " 'recruiting',\n",
       " 's_a__trading',\n",
       " 'so2',\n",
       " 'social',\n",
       " 'stored_messages',\n",
       " 'swap_group_project_lists',\n",
       " 'tasks',\n",
       " 'time_off',\n",
       " 'tnrcc',\n",
       " 'to_do',\n",
       " 'training',\n",
       " 'travel',\n",
       " 'trevor',\n",
       " 'uk_trading',\n",
       " 'yoder'}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2852, 4000)\n"
     ]
    }
   ],
   "source": [
    "# from here on we will use the sklearn\n",
    "# which does not seem to have stemming, but for now we won't use it\n",
    "# form my experience stemming does not always increase performance\n",
    "\n",
    "# vectorizing\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=4000,strip_accents='unicode',lowercase=True)\n",
    "\n",
    "# Construct the required TF-IDF matrix by fitting the transformation model\n",
    "# and then applying the transformation\n",
    "vectorizer.fit(X_train)\n",
    "vect_matrix_train = vectorizer.transform(X_train)\n",
    "vect_matrix_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Output the shape of tfidf_matrix of test data set\n",
    "print(vect_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier and predict the training and test data\n",
    "# maybe using complement NB to deal with unbalancedness\n",
    "#clf = MultinomialNB()\n",
    "clf = ComplementNB()\n",
    "#clf = GaussianNB()\n",
    "clf.fit(vect_matrix_train, y_train)\n",
    "preds_ts = clf.predict(vect_matrix_test)\n",
    "preds_tr = clf.predict(vect_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Train:\n",
      "Accuracy: 0.527706\n",
      "Accuracy under assumption of null-hypothesis: 0.263183\n",
      " \n",
      "Performance Test:\n",
      "Accuracy: 0.434081\n",
      "Accuracy under assumption of null-hypothesis: 0.274148\n"
     ]
    }
   ],
   "source": [
    "# evaluation train and test\n",
    "print('Performance Train:')\n",
    "report(y_train, preds_tr)\n",
    "print(' ')\n",
    "print('Performance Test:')\n",
    "report(y_test, preds_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "acc_0 = 0\n",
    "for i in range(0,100):\n",
    "    random.shuffle(b)\n",
    "    acc_0 += np.mean(a==b)\n",
    "\n",
    "print(acc_0/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 17, 0, 15, 12, 6, 7, 13, 10, 14, 4, 2, 18, 3, 19, 8, 9, 1, 5, 11]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
